---
layout: post
title: 机器学习系列（十）- L1和L2正则化（一）
date: 2022-01-10
categories: 机器学习系列
tags: 研究生 机器学习

---

# 机器学习系列（十）- $L1$ 和 $L2$ 正则化（一）

- 视频参考：[“L1和L2正则化”直观理解(之一)，从拉格朗日乘数法角度进行理解](https://www.bilibili.com/video/BV1Z44y147xA)
- 知乎参考：[L1正则化与L2正则化](https://zhuanlan.zhihu.com/p/35356992)、[L1和L2 详解(范数、损失函数、正则化)](https://zhuanlan.zhihu.com/p/137073968)
- CSDN参考：[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)
- 博客参考：[你真的理解正则化了吗？](http://imgtec.eetrend.com/blog/2019/100046445.html)
- 拓展阅读：[L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸](https://zhuanlan.zhihu.com/p/205023954)

## 基础概念

- **范数：**$L1$ 和 $L2$ 正则化，其中 $L1$ 和 $L2$ 指代 $1$ 范数和 $2$ 范数；当范数 $p \ge 1$ 时，范数可行域为凸集，可以关联至拉格朗日对偶函数的凸优化问题； 

  ![image-20220110161547232](http://markdown.zzzbook.cn/image-20220110161547232.png)

- **模型参数的非唯一性：**当模型的损失函数值确定时，对应的模型参数 $W$ 和 $b$ 可以有多种取值；

  如当我们将隐藏层中每层参数扩大 $2$ 倍，即 $W^{l}=2·W^{l}$ 与 $b^{l}=2·b^{l}$ ，则为保证输出层结果不变，输出层输入修改为

  $z^{l}=W^{l^{T}}·a^{l-1}+b^{l} \Rightarrow (\frac{1}{2^{l-1}}W^{l^{T}})·2^{l-1}·a^{l-1}+b^{l}$ 

  其中 $2^{l-1}$ 指代我们将上层输入 $a^{l-1}$ 累积扩大的倍数（ $l-1$ 层），为保证最终结果不变，将模型参数 $W^{l}$ 缩小 $2^{l-1}$ 倍，进而导致在相同损失取值的情况下 $W$ 和 $b$ 有多种取值；

## 正则化概念

- **函数复杂度**

  由下图可以看出，曲线 $y_2$、$y_3$ 相对平缓，而 $y_1$ 函数变化值较大；通过对比函数式可知 $y_1$ 的**自变量系数较大**；通常，如果函数的取值变化幅度更大，我们认为函数更复杂，函数的方差更大；

  <img src="http://markdown.zzzbook.cn/100046445-86163-11.png" alt="100046445-86163-11" style="zoom:50%;" />

- **模型过拟合**

  如下图所示，就是过拟合的情况，拟合函数考虑到了每一个样本点，最终形成的拟合函数波动很大，也就是**在某些很小的区间里，函数值的变化很剧烈**。这就意味着函数在某些小区间里的**系数**非常大，就是模型中的 $W$ 会很大；

  理解：防止过拟合是为了使图像更加平缓；

  ![v2-b1bd7c5ea5c6de5ce8967fd71b4970dd_720w](http://markdown.zzzbook.cn/v2-b1bd7c5ea5c6de5ce8967fd71b4970dd_720w.jpg)

- **正则化如何避免过拟合**

  正则化通过**降低模型的复杂性**缓解过拟合；过拟合发生的情况 $W$ 往往非常大，噪声、误差也会被放大很多；拟合过程中倾向于让权值 $W$ 尽可能小，最后构造一个所有参数都比较小的模型；一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象；

  **理解：如下图，正则化通过限制 $W$ 的取值范围，避免 $W$ 在相同拟合情况下，取到较大的值；**

  <img src="http://markdown.zzzbook.cn/v2-2e038f7a54e885714a7bf5ab9c5dda56_1440w.jpg" alt="v2-2e038f7a54e885714a7bf5ab9c5dda56_1440w" style="zoom: 33%;" />

- **正则化后最优点与正则化前不一致，且存在一定梯度差异如何理解？**

  个人理解：

  1. 若当模型对训练集拟合准确率达到 $100\%$ 时，则相当于模型梯度下降至最低点（等高线中心点），本质上该情况就是过拟合；正则化防止过拟合应该是在一定程度上牺牲训练模型的精度，而提升泛化能力；则训练模型损失的精度，应该为正则化前最优点与正则化后最优点之间的梯度差异；目标是寻找训练损失精度与正则化限制条件之间的平衡值，已达到拟合与泛化的最优平衡值；
  
     <img src="http://markdown.zzzbook.cn/image-20220110155957162.png" alt="image-20220110155957162" style="zoom:67%;" />
  
  2. 根据模型参数的非唯一性，同损失值将对应不同的权重参数，如图中虚线为同损失值对应的不同权重参数组成集合；故当正则化后最值点将与正则化前最值点对应的权重参数中，离其最近的权重参数点做差值，即蓝点做虚线垂线的距离；所以梯度差异并非两最值点直接连线的较大差异；
  
     <img src="http://markdown.zzzbook.cn/image-20220111215422901.png" alt="image-20220111215422901" style="zoom: 67%;" />

## $L1$ 和 $L2$ 正则化

- 定义公式：$\begin{array}{l} minJ(W,b,x) \newline s.t. \Vert W \Vert_{1/2}-C\le 0 \end{array}$ 

- 拉格朗日乘子式：$\begin{array}{l} L(W,\lambda)=J(W)+\lambda(\Vert W \Vert_{1/2}-C) \newline min_{W}max_{\lambda}L(W,\lambda) \newline s.t. \lambda \ge 0 \end{array}$  

  <img src="http://markdown.zzzbook.cn/v2-2e038f7a54e885714a7bf5ab9c5dda56_1440w.jpg" alt="v2-2e038f7a54e885714a7bf5ab9c5dda56_1440w" style="zoom: 33%;" />

- 正则化公式：$L(W,\lambda)=J(W)+\lambda\Vert W \Vert_{1/2}$ 

### 正则化公式推导

1. 定义公式：

   $\begin{array}{l} L_{a}(W,\lambda)=J(W)+\lambda(\Vert W \Vert_{1/2}-C)= & J(W)+\lambda \Vert W \Vert_{1/2}-\lambda C \newline L_{b}(W,\lambda)= & J(W)+\lambda\Vert W \Vert_{1/2} \end{array}$ 

2. 当对 $W$ 求导时 $\nabla_{W}L_{a}(W,\lambda)=\nabla_{W}L_{b}(W,\lambda)=0$ ，因 $\lambda C$ 为常数，故导数相等；

   所以 $arg_{w} \begin{pmatrix} min_{W}max_{\lambda}L_{a}(W,\lambda) \newline s.t.\lambda\ge 0 \end{pmatrix}=arg_{w} \begin{pmatrix} min_{W}max_{\lambda}L_{b}(W,\lambda) \newline s.t.\lambda\ge 0 \end{pmatrix}$ ；

   虽然两公式极值不同，但导数相同，即 $W$ 取值相同；

   注：$arg$ 为当 $f(x)$ 取最大值或最小值时，$x$ 取值；

3. 拉格朗日乘子式，增加参数 $\lambda、C$ ，其中 $C$ 被消除；

   在式中 $C$ 决定范数可行域大小，以 $2$ 范数为例，决定了可行域的半径；

   如图所示，红色向量指代 $J(W)$ 的梯度，绿色向量指代 $\lambda(\Vert W \Vert_{1/2}-C)$ 的梯度，为使导数等于 $0$ ，需要通过调整 $\lambda$ 来确定绿色向量大小；当乘子式 $\lambda$ 确定，则可行域半径确定，则 $C$ 也被唯一确定；故参数 $C$ 受参数 $\lambda$ 影响并唯一确定；

   <img src="http://markdown.zzzbook.cn/image-20220110194025543.png" alt="image-20220110194025543" style="zoom:67%;" />

### 正则化特性

#### $L1$ 正则化

- $L1$ 正则常被用来进行特征选择，主要原因在于 $L1$ 正则化会使得较多的参数为 $0$，从而产生稀疏解，我们可以将 $0$ 对应的特征遗弃，进而用来选择特征。一定程度上 $L1$ 正则也可以防止模型过拟合；

- **特征选择原理：**

  当范数可行域与某条等高线相切（仅有一个交点）的时候，可行域最小；由下图可知交点在范数可行域边界取值时，当移动到切点时取值最优；故 $1$ 范数会使部分特征取值为 $0$，则容易得到稀疏解；

  <img src="http://markdown.zzzbook.cn/image-20220110201251362.png" alt="image-20220110201251362" style="zoom: 50%;" />

#### $L2$ 正则化

-  $L2$ 正则主要用来防止模型过拟合，直观上理解就是对于大数值的权重向量进行惩罚，鼓励参数是较小值；

  <img src="http://markdown.zzzbook.cn/image-20220110201312834.png" alt="image-20220110201312834" style="zoom:50%;" />
