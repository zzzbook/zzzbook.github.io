---
layout: post
title: 机器学习系列（五）- 交叉熵
date: 2021-10-13
categories: 机器学习系列
tags: 研究生 机器学习

---

# 机器学习系列（五）- 交叉熵

- 视频参考：[“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV15V411W7VB)

## 基础概念

- **公度**：不可直接进行比对的量，转换为同度量单位。在神经网络中，当两个模型不同时无法直接进行比对，则将模型转换为统一度量 - “熵”。
- **熵**：表示一个系统中的不确定性的值。
- **信息量**：消除系统中不确定性的量。

## 定义信息量

- **推导过程**

  1. 定义信息量 $f(p(x))$ ，假定 $x$ 为最终结果， $x_1,x_2$ 为独立的前置条件；

  2. 则存在信息量 $f(p(x))=f(p(x_1))+f(p(x_2))$ ；

  3. 根据概率定义 $p(x)=p(x_1)·p(x_2)=p(x_1x_2)$ ；

  4. 通过公式转换，结果为 $f(p(x_1x_2))=f(p(x_1))+f(p(x_2))$ ；

  5. 为满足推断公式，则定义 $f(p(x)):=-log_2p(x)$；

     - 为使乘法转变为加法，故引入 $log$ 函数；

     - 由于 $log$ 单调递增，但当 $p(x)$ 越小所得到的信息量越大，即越不确定的事情发生所得到的信息量越大，故增加负号；

     - 为将所得数值以比特（byte）为单位，故 $log$ 以 2 为底，便于换算；
     
       如当计算机存储16位数据，在输入前任意结果概率为 $\frac{1}{2^{16}}$ ，当输入后概率为1，则信息量为16 byte；

### 定义交叉熵

1. **熵**是衡量整个系统中所有事件的信息量，熵越大代表系统中事件的不确定性越高；

   如系统 $Z$ 事件 $a,b$ 发生的概率分别为 $50\%$ ，系统 $Y$ 中 $c,d$ 发生的概率分别为 $99\%,1\%$ ，当结果确定前系统 $Z$ 的不确定性高，所以熵越大；若单纯将信息量相加不满足规律，故需要进行加权运算；

   <table>
       <tr>
           <td>系统</td>
           <td colspan="2">$Z$</td>
           <td colspan="2">$Y$</td>
       </tr>
       <tr>
           <td>事件</td>
           <td>$a$</td>
           <td>$b$</td>
           <td>$c$</td>
           <td>$d$</td>
       </tr>
       <tr>
           <td>概率</td>
           <td>$\frac{1}{2}$</td>
           <td>$\frac{1}{2}$</td>
           <td>$\frac{99}{100}$</td>
           <td>$\frac{1}{100}$</td>
       </tr>
       <tr>
           <td>信息量</td>
           <td>$-log_2 \frac{1}{2} =1$</td>
           <td>$-log_2 \frac{1}{2} =1$</td>
           <td>$-log_2 \frac{99}{10}\approx 0.0145$</td>
           <td>$-log_2 \frac{1}{10}\approx 6.6439$</td>
       </tr>
       <tr>
           <td>对系统贡献信息量</td>
           <td>$\begin{align}\frac{1}{2}*1 \newline =0.5\end{align}$</td>
           <td>$\begin{align}\frac{1}{2}*1 \newline =0.5\end{align}$</td>
           <td>$\begin{align}\frac{99}{10}(-log_2 \frac{99}{10}) \newline \approx 0.014355\end{align}$</td>
           <td>$\begin{align}\frac{99}{10}(-log_2 \frac{1}{10}) \newline \approx 0.066439\end{align}$</td>
       </tr>
       <tr>
           <td>熵</td>
           <td colspan="2">$1$</td>
           <td colspan="2">$\approx 0.080794$</td>
       </tr>
   </table>

2. **数学期望**指试验中每次可能的结果的概率乘以其结果的总和，与我们对于熵的定义一致；

   则我们定义熵为对于信息量的期望，即

   $\begin{align}H(P) :&=E(P_f)=\sum_{i=1}^mp_i·f(p_i)=\sum_{i=1}^mp_i(-log_2p_i)=-\sum_{i=1}^mp_i·log_2p_i\end{align}$ 

3. **相对熵**，也称为KL散度或信息散度。参与计算的一个概率分布为真实分布，另一个为理论（拟合）分布，相对熵表示使用理论分布拟合真实分布时产生的信息损耗，公式如下：

   $\begin{align}D_{KL}(P\Vert Q):&=\sum_{i=1}^mp_i·(f_Q(q_i)-f_P(p_i))\newline & =\sum_{i=1}^mp_i·((-log_2q_i)-(-log_2p_i))\newline &=\sum_{i=1}^mp_i·(-log_2q_i)-\sum_{i=1}^mp_i·(-log_2p_i)\end{align}$ 

   该定义为以 $P$ 作为基准，讨论 $P$ 与 $Q$ 之间的损耗，当两个模型相等时，即 $f_Q(q_i)=f_P(p_i)$ ，相对熵 $D_{KL}=0$ ，故我们期望 $D_{KL}$ 越小越好；

   最后等式中 $\sum_{i=1}^mp_i·(-log_2p_i)=E(P_i)=H(P)$ 即为 $P$ 的熵；

4. **交叉熵**则为 $\sum_{i=1}^mp_i·(-log_2q_i)=H(P,Q)$ ；

5. **吉布斯不等式** 相结合

   - 若 $\sum_{i=1}^np_i=\sum_{i=1}^nq_i=1$ ，且 $p_i,q_i\in(0,1]$ 

     则有 $-\sum_{i=1}^np_ilogp_i\le-\sum_{i=1}^np_ilogq_i$ 

     等号成立当且仅当 $p_i=q_i\forall i$ 

   可知一定存在 $D_{KL} \ge 0$ ，则 $H(P,Q)$ 越小则越接近真实模型。

6. **交叉熵结合模型** $H(P,Q)=\sum_{i=1}^mp_i·(-log_2q_i)$ 

   <img src="http://r0umepz0y.hb-bkt.clouddn.com/img/image-20211012134020697.png" alt="image-20211012134020697" style="zoom:50%;" />

   - 针对二分类问题，则 $m\to n=2$ ， $p_i\to x_i\in\lbrace0,1\rbrace$ ， $q_i\to y_i$ 为当 $x_i$ 取值时模型对应正确概率，故转变为

     $H(x,y)=-\sum_{i=1}^2p_i·log_2q_i=-(x_i·(-log_2y_i)+(1-x_i)·log(1-y_i))$ 

     模型系统内交叉熵则为：$-\sum_{i=1}^m(x_i·(-log_2y_i)+(1-x_i)·log(1-y_i))$ ， $m$ 为样本数量；

   - 针对多分类问题，当 $x_i=0$ 时交叉熵 $\neq0$ ，当 $y_i=0$ 时交叉熵 $=0$ ，故 $m$ 取值为模型的可分类数；

     如模型可分类 $a,b,c$ ，传入类型 $d$ 则 $y_i=0$ ，故与 $x_i$ 类型数量无关，与 $y_i$ 类型数量有关；

     故转变为 $H(x,y)=-\sum_{i=1}^mp_i·log_2q_i=-\sum_{i=1}^nx_i·log_2y_i$ 

     模型系统内交叉熵则为：$-\sum_{k=1}^m\sum_{i=1}^nx_{m,i}·log_2y_{m,i}$ ， $m$ 为样本数量；

7. **损失函数**则为获取交叉熵最小值，即 $min-\sum_{i=1}^m(x_i·(-log_2y_i)+(1-x_i)·log(1-y_i))$ ，与极大似然估计一致；

