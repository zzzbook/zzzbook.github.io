---
layout: post
title: 机器学习系列（六）- 梯度下降法
date: 2021-10-13
categories: 机器学习系列
tags: 研究生 机器学习

---

# 机器学习系列（六）- 梯度下降法

- 视频参考：[如何理解“梯度下降法”？什么是“反向传播”？通过一个视频，一步一步全部搞明白](https://www.bilibili.com/video/BV1Zg411T71b)
- 知乎参考：[梯度下降法在简单神经网络中的应用](https://zhuanlan.zhihu.com/p/269615620)
- CSDN参考：[神经网络之梯度下降法及其实现](https://blog.csdn.net/nanhuaibeian/article/details/100184893)

## 基础概念

- **梯度下降法**目标为寻找**局部最优解**；在一元函数梯度代表着给定点切线的斜率，在多元函数中梯度代表一个向量；梯度方向指出函数在给定点上升最快的方向，即 $\nabla J$ ；则梯度的反方向则是函数下降最快的方向，即 $-\nabla J$ ；
- **学习步长**是一个用于调整梯度更新速度的参数，合适的大小使模型更容易收敛；学习步长太小，算法收敛会很慢；学习步长太大，可能造成算法不收敛甚至发散。

## 公式推导

1. 设 $\sigma(x)$ 为感知机的激活函数；则感知机结果为 $a^{[n]}=\sigma(W^{[n]}·a^{[n-1]}+b^{[n]})$ ；

   其中 $W$ 、$b$ 都会产生损失值， $a$ 则由 $n-1$ 层决定损失值。

