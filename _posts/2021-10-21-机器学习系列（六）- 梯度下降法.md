---
layout: post
title: 机器学习系列（六）- 梯度下降法
date: 2021-10-13
categories: 机器学习系列
tags: 研究生 机器学习

---

# 机器学习系列（六）- 梯度下降法

- 视频参考：[如何理解“梯度下降法”？什么是“反向传播”？通过一个视频，一步一步全部搞明白](https://www.bilibili.com/video/BV1Zg411T71b)
- 知乎参考：[梯度下降法在简单神经网络中的应用](https://zhuanlan.zhihu.com/p/269615620)
- CSDN参考：[神经网络之梯度下降法及其实现](https://blog.csdn.net/nanhuaibeian/article/details/100184893)

## 基础概念

- <span id="gradient">**梯度下降法**目标为寻找**局部最优解**；在一元函数梯度代表着给定点切线的斜率，在多元函数中梯度代表一个向量；梯度方向指出函数在给定点上升最快的方向，即 $\nabla J$ ；则梯度的反方向则是函数下降最快的方向，即 $-\nabla J$ ；</span>
- **学习步长**是一个用于调整梯度更新速度的参数，合适的大小使模型更容易收敛；学习步长太小，算法收敛会很慢；学习步长太大，可能造成算法不收敛甚至发散。

## 梯度理解

1. <a href="#gradient">一元函数梯度即斜率</a>；在多元函数中对每个变量求偏导，即每个变量的斜率，切线构成平面则为函数切面即最大梯度，如下图所示。

<img src="http://markdown.zzzbook.cn/image-20211130100441151.png" alt="image-20211130100441151" style="zoom:67%;" />

2. 将切面图从 $Z$ 轴俯视，则构成如下图所示坐标系，可清晰的看出两条切线作为向量相加即为所求最大梯度方向。

   <img src="http://markdown.zzzbook.cn/image-20211130101010735.png" alt="image-20211130101010735" style="zoom:67%;" />

## 公式推导

1. 设 $\sigma(x)$ 为感知机的激活函数；则感知机结果为 $a^{[n]}=\sigma(z^{[n]})=\sigma(W^{[n]}·a^{[n-1]}+b^{[n]})$ ；

   其中 $W$ 、$b$ 都会产生损失值， $a$ 则由 $n-1$ 层决定损失值。
   
   
   
   test $\cdots$
   
   
   
   层感知机计算公式：$z^{[l]} = \begin{bmatrix} z^{[l]}_1 \newline \vdots \newline z^{[l]}_i \end{bmatrix} = \begin{bmatrix} W^{[l]}_{1,1} & \cdots & W^{[l]}_{1,j} \newline \vdots & \ddots & \vdots \newline W^{[l]}_{i,1} & \cdots & W^{[l]}_{i,j} \end{bmatrix} · \begin{bmatrix} a^{[l-1]}_{1} \newline \vdots \newline a^{[l-1]}_{j}\end{bmatrix} + \begin{bmatrix} b^{[l]}_{1} \newline \vdots \newline b^{[l]}_{i} \end{bmatrix}$ 
   
   <img src="http://markdown.zzzbook.cn/image-20211130142728657.png" alt="image-20211130142728657" style="zoom:50%;" />
   
2. 定义损失函数 $-\nabla J^{[n+1]}_i(W^{[n]}_i, a^{[n-1]}_i, b^{[n]}_i)$ ，$\eta$ 为学习步长，其中 

   $W^{[n]}_{new.i} = W^{[n]}_{old.i} - \eta · \nabla{W}^{[n]}_i = W^{[n]}_{old.i} - \eta · \frac{\mathrm{d} J^{[n+1]}_i }{\mathrm{d} W^{[n]}_i}$ 

   $b^{[n]}_{new.i} = b^{[n]}_{old.i} - \eta · \nabla b^{[n]}_i = b^{[n]}_{old.i} - \eta · \frac{\mathrm{d} J^{[n+1]}_i }{\mathrm{d} b^{[n]}_i}$ 

   <img src="http://markdown.zzzbook.cn/image-20211130142648010.png" alt="image-20211130142648010" style="zoom:50%;" />

3. 反向传播递归，推导递归为

    $\nabla J^{[n]} = \frac{\mathrm{d} J^{[n+1]} }{\mathrm{d} a^{[n-1]}} = \nabla J^{[n]}_i(W^{[n-1]}_i, a^{[n-2]}_i, b^{[n-1]}_i) \Rightarrow \nabla J^{[n+1]} = \nabla J^{[n+1]}_i(W^{[n]}_i, a^{[n-1]}_i, b^{[n]}_i)$ （）

   ，其中 $W$ 递归推导（ $b$ 同理）

   $ & W^{[n-1]}_{new.i} = W^{[n-1]}_{old.i} - \eta · \nabla{W}^{[n-1]}_i = W^{[n-1]}_{old.i} - \eta · (\frac{1}{m} \sum^{m}_{k=1}\frac{\mathrm{d} J^{[n]}_k }{\mathrm{d} W^{[n-1]}_i}) \newline \Rightarrow & W^{[n]}_{new.i} = W^{[n]}_{old.i} - \eta · \nabla{W}^{[n]}_i = W^{[n]}_{old.i} - \eta · (\frac{1}{m} \sum^{m}_{k=1}\frac{\mathrm{d} J^{[n+1]}_k }{\mathrm{d} W^{[n]}_i})$ 

4. 当反向传播递归至最后一层， $a^{[0]}$ 为输入样本，损失函数为 $\nabla J^{[1]}_i(W^{[1]}_i, b^{[1]}_i)$ 
