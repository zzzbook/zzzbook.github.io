---
layout: post
title: 机器学习系列（七）- softmax 最大熵
date: 2021-12-03
categories: 机器学习系列
tags: 研究生 机器学习

---

# 机器学习系列（七）- $softmax$ 最大熵

- 视频参考：[softmax是为了解决归一问题凑出来的吗？和最大熵是什么关系？最大熵对机器学习为什么非常重要？](https://www.bilibili.com/video/BV1cP4y1t7cP)
- 知乎参考：[如何理解最大熵模型里面的特征？](https://www.zhihu.com/question/24094554/answer/1057861756)
- 博客参考：[最大熵模型](https://transwarpio.github.io/teaching_ml/2017/08/15/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/)

## $sigmod$ 函数缺点

- $sigmod$ 函数在末端存在梯度消失问题，导致反向转播优化参数过程中效率降低，可更换为 $ReLU$ 函数。

- $sigmod$ 函数无法解决多分类问题，只能解决但分类问题。

  <img src="http://markdown.zzzbook.cn/image-20211203202343427.png" alt="image-20211203202343427" style="zoom:50%;" />

  <img src="http://markdown.zzzbook.cn/image-20211203202425478.png" alt="image-20211203202425478" style="zoom:50%;" />

## $Softmax$ 函数

- $softmax$ 函数主要用于解决多分类问题。

### 公式推导

1. 公式：$z^{[l][k]}=W^{[l][k]}·a^{[l-1][k]}+b^{[l][k]}$ ，其中 $k$ 对应每个分类

   激活函数：$a^{[l][k]}= \sigma(z^{[l][k]})$ 

2. 多分类问题应输出每个分类的概率，故针对多分类问题要求满足：

   $a^{[l][k]}= \begin{bmatrix} a_1^{[l][k]} \newline \vdots \newline a_n^{[l][k]} \end{bmatrix}$ ，其中 $a_i^{[l][k]} \ge 0$ 且 $\sum_{i=1}^na_i^{[k][k]}=1$ 

3. 为满足 $a_i^{[l][k]} \ge 0$ ，取 $a^{[l][k]}$ 以 $e$ 为底，即

   $\sigma(z^{[l][k]}) = e^{z^{[l][k]}} = \begin{bmatrix} e^{z_1^{[l][k]}} \newline \vdots \newline e^{z_n^{[l][k]}} \end{bmatrix} = \begin{bmatrix} t_1^{[k]} \newline \vdots \newline t_n^{[k]} \end{bmatrix}$ 

   为满足 $\sum_{i=1}^na_i^{[k][k]}=1$ ，同除总和，<span id="max">即</span> 

    $\sigma(z^{[l][k]}) = softmax(z^{[l][k]}) = \begin{bmatrix} t_1^{[k]}/\sum_{i=1}^n t_j^{k} \newline \vdots \newline t_n^{[k]}/\sum_{i=1}^n t_j^{k} \end{bmatrix} = \begin{bmatrix} p_1^k \newline \vdots \newline p_n^k \end{bmatrix}$ 

4. 交叉熵损失函数变化：

    $\begin{align} H(Q)=-\sum_{i=1}^n q_i·log_2p_i \Rightarrow H(Q,P)=J=-\sum_{k=1}^m\sum_{i=1}^n q_i^{[k]}·log_2p_i^{[k]} \end{align}$ 

### $sigmod$ 特殊的 $softmax$ 

1.  $\begin{align} sigmod(z^{[l][k]}) =\frac{1}{1+e^{-z^{[l][k]}}} =\frac{1}{1+\cfrac{1}{e^{-z^{[l][k]}}}} =\frac{e^{-z^{[l][k]}}}{e^{-z^{[l][k]}}+1} \end{align}$ 
2. 令 $\begin{bmatrix} t1 \newline t2 \end{bmatrix} = \begin{bmatrix} e^{[l][k]} \newline e^0 \end{bmatrix} = \begin{bmatrix} e^{[l][k]} \newline 1 \end{bmatrix}$ 
3. 则 $\begin{align} sigmod(z^{[l][k]}) =\frac{t_1}{t_1+t_2} \end{align}$； $\begin{align} 1-sigmod(z^{[l][k]}) = \frac{1}{t_1+t_2} = \frac{t_2}{t_1+t_2}\end{align}$ 

### 单分类 $sigmod$ 与二分类 $softmax$ 

- 单分类 $sigmod$ 产生两个概率结果，一个为“是”的概率，一个未“不是”的概率

  二分类 $softmax$ 同样产生两个概率结果，一个为“第一分类”的概率，一个为“第二分类”的概率

- 单分类 $sigmod$ 与二分类 $softmax$ 公式：

   $t=\begin{bmatrix} t1 \newline t2 \end{bmatrix} = \begin{bmatrix} e^{[l][k]} \newline e^0 \end{bmatrix} = \begin{bmatrix} e^{[l][k]} \newline 1 \end{bmatrix} & \Rightarrow J = -\sum_{k=1}^my_1^k·logp_1^k+(1-y_1^k)·logp_2^k$ 

  $t=\begin{bmatrix} t1 \newline t2 \end{bmatrix} = \begin{bmatrix} e_1^{[l][k]} \newline e_2^{[l][k]} \end{bmatrix} & \Rightarrow J = -\sum_{k=1}^my_1^k·logp_1^k+y_2^k·logp_2^k$ 

- 单分类问题中，$t_2$ 固定为 $1$ ，概率受 $t_1$ 影响；二分类问题中，由 $t_1,t_2$ 共同作用；

## 最大熵

### 模型对比

- 正态分布 $N(\mu,\sigma^2)$ 
  - 期望：$\mu=\textcolor{Red}{E[x]}$ 
  - 方差：$\sigma^2=E[(x-\mu)]^2=E[x^2]-E^2[x]=\textcolor{Red}{E[x^2]}-\mu^2$ 
  - 偏度：$偏度=E\begin{bmatrix}(\cfrac{x-\mu}{\sigma})^3\end{bmatrix}=\cfrac{E[(x-\mu)^3]}{E[\sigma^3]}=\cfrac{E[x^3]-3\mu E[x^2]+2\mu^3}{\sigma^3}=\cfrac{\textcolor{Red}{E[x^3]}-3\sigma^2+2\mu^2}{\sigma^3}$ 

<img src="http://markdown.zzzbook.cn/image-20211206145117572.png" alt="image-20211206145117572" style="zoom:50%;" />

- 其中 $E[x]$ 为一阶矩， $E[x^2]$ 为二阶矩， $E[x^3]$ 为三阶矩。可知概率密度分布特征与矩密切相关。

- 概率分布与特征函数一一对应（泰勒展开）*（参照[视频23分钟](https://www.bilibili.com/video/BV1cP4y1t7cP)）*：

  $\begin{align} & \varphi_x(t)=E(e^{itx}) \newline \Rightarrow & E[1]+E[\cfrac{itx}{1}]-E[\cfrac{t^2x^2}{2!}] + \cdots + E[\cfrac{(it)^nx^n}{n!}] \newline \Rightarrow & 1+\cfrac{it\textcolor{Red}{E(x)}}{1}-\cfrac{t^2\textcolor{Red}{E(x^2)}}{2!}+\cdots+\cfrac{(it)^n\textcolor{Red}{E(x^n)}}{n!} \end{align}$ 

- 当 $f(x)= \begin{bmatrix} x \newline x^2 \newline x^3 \newline \vdots \end{bmatrix}$ ，若 $E_{\varphi}[f(x)]=\begin{bmatrix} E_{\varphi}(x) \newline E_{\varphi}(x^2) \newline E_{\varphi}(x^3) \newline \vdots \end{bmatrix}，E_{p}[f(x)]=\begin{bmatrix} E_{p}(x) \newline E_{p}(x^2) \newline E_{p}(x^3) \newline \vdots \end{bmatrix}$ 

  则当 $E_{\varphi}[f(x)]=E_{p}[f(x)]$ 时，可证明两概率分布相同。

### 先验概率（经验概率）

- 经验联合分布：$\tilde{P}(x,y)=\cfrac{count(x,y)}{Count}$ ，经验边缘分布：$\tilde{P}(x)=\cfrac{count(x)}{Count}$ 

- $P(y|x)$ ，当输入样本 $x$ 时，输出分类 $y$ 的概率；

  目标使 $P(y|x)$ 熵最大，且 $E_p(f(x))=E_\tilde{p}(f(x))$ 

- 令 $P(x,y)=\tilde{P}(x,y)=P(x)·P(y|x)\approx \tilde{P}(x)·P(y|x)$ 

### 特征函数（指示函数）

- 在样本空间 $\{(x^k,y^k)\}$ 上，设计随机变量 $x=\begin{cases} 1, & (x,y)满足事件m \newline 0, & 否则 \end{cases}$ 

  其中 $m$ 覆盖了样本空间（训练集）中所有的可能。*（原因参照[知乎答案](https://www.zhihu.com/question/24094554/answer/1057861756)）*

- 则 $E(x)=\tilde{P}(x)·1+(1-\tilde{P}(x))·0=\tilde{P}(A)$ 等于对应事件发生的概率。

- 目标将多维数据降至一维，使用伯努利分布只需要判断一阶矩。设 $f(x,y)=\begin{bmatrix} x_1 \newline x_2 \newline \vdots \newline x_n \end{bmatrix}$ 

### 条件熵

- 熵的定义：$H(x):=-\sum_{i=1}^np(x)·log_2p(x)$ 

- $H^{(m_1)}=-\sum_{i=1}^np(y_i^{(m_1)}|x^{(m_1)})·log_2p(y_i^{(m_1)}|x^{(m_1)})$ 为事件 $m_1=(x^{(1)},y_1^{(1)},y_2^{(1)},\cdots,,y_n^{(1)})$ 的熵；

  则事件 $m$ 的熵为：

  $\begin{align}H(Y|X)=\sum_{k=1}^mp(x^{(m)})·H^{(m)}:=-\sum_{x,y}p(x)p(y|x)log_2p(y|x)\end{align}=E(H(Y|X=x^{[k]}))$ 

### 最大熵推导

- 令 $p(x)=\tilde{p}(x)$ ，将 $max$ 转变为 $min$ ，则

  $\begin{align} -max\sum_{x,y}p(x)p(y|x)log_2p(y|x) \Rightarrow min\sum_{x,y}\tilde{p}(x)p(y|x)log_2p(y|x) \end{align}$ 

- 限制条件：$\begin{cases} E_{\tilde{p}}(f(x,y))-E_{p}(f(x,y)) \newline \Rightarrow \Delta_k-\sum_{x,y}p(x,y)f_k(x,y) \newline \Rightarrow \Delta_k-\sum_{x,y}\tilde p(x)p(y|x)f_k(x,y)=0,&即模型相同 \newline \sum_yp(y|x)=1 \newline \Rightarrow 1-\sum_yp(y|x)=0,&即概率和为1 \end{cases}$ 

- 利用拉格朗日乘数法

  $\begin{align} L(P,\lambda)=\sum_{x,y}\tilde{p}(x)p(y|x)log_2p(y|x)+\lambda_0\left(1-\sum_yp(y|x)\right)+\sum_{k=1}^m\lambda_k\left(\Delta_k-\sum_{x,y}\tilde{p}(x)p(y|x)f_k(x,y)\right) \end{align}$ 

- 求导所得：

  $\begin{align} \cfrac{\partial L(P,\lambda)}{\partial P(y|x)} & =\sum_{x,y}\tilde{p}(x)\left(log_2p(y|x)+1\right)-\sum_y\lambda_0-\sum_{k=1}^m\lambda_k\left(\sum_{x,y}\tilde{p}(x)f_k(x,y)\right) \newline & \Rightarrow \sum_{x,y}\tilde{p}(x)\left(log_2p(y|x)+1\right)-\sum_x\tilde{p}(x)\sum_y\lambda_0-\sum_{x,y}\tilde{p}(x)\sum_{k=1}^m\lambda_k·f_k(x,y) \newline & \Rightarrow \sum_{x,y}\tilde{p}(x)\left(log_2p(y|x)+1\right)-\sum_{x,y}\tilde{p}(x)\lambda_0-\sum_{x,y}\tilde{p}(x)\sum_{k=1}^m\lambda_k·f_k(x,y) \newline & \Rightarrow \sum_{x,y}\tilde{p}(x)\left(log_2p(y|x)+1-\lambda_0-\sum_{k=1}^m\lambda_k·f_k(x,y)\right) \newline & = 0 \end{align}$  

- 推导可得：$log_2p(y|x)+1-\lambda_0-\sum_{k=1}^m\lambda_k·f_k(x,y)=0$ 

  $ p(y|x) & =e^{\lambda_0-1}e^{\sum_{k=1}^m\lambda_k·f_k(x,y)} \newline & \Rightarrow \cfrac{e^{\sum_{k=1}^m\lambda_k·f_k(x,y)}}{e^{1-\lambda_0}}$ 

- $\sum_{k=1}^m\lambda_k·f_k(x,y)$ 由向量表示可得 $\eta^T·f(x,y)$ ，则 $e^{\sum_{k=1}^m\lambda_k·f_k(x,y)} = e^{\eta^T·f(x,y)}$ 

  由 $\sum_yp(y|x)=1$ 可知 $e^{1-\lambda_0}=\sum_ye^{\eta^T·f(x,y)}$ 

  所以可得 $p(y|x)=\cfrac{e^{\eta^T·f(x,y)}}{\sum_ye^{\eta^T·f(x,y)}}$ 

- 令 $e^{\eta^T·f(x,y)} = t_n$ ，则 $p(y|x)=t_n/\sum_{i=1}^n t_j$ ，则<a href="#max">同上述推导</a> 





